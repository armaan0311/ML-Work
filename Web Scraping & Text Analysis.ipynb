{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rqLqPGcQ5tU2OboiFLtpWVo1upv-QHPt","authorship_tag":"ABX9TyPnJxW72nbAXdwyrhos+o8N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","\n","def fetch_article(url):\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        content_div = soup.find('div', class_=lambda x: x and 'td-post-content' in x.split())\n","\n","        # Extract the article title\n","        title = soup.find('h1').get_text()\n","\n","        # Extract text from paragraphs and list items within ol tags\n","        paragraphs = content_div.find_all(['p', 'ol', 'ul'])\n","\n","        # Creating article dir for extracted text\n","        article_text = ''\n","        for para in paragraphs:\n","            if para.name == 'p':\n","                article_text += para.get_text() + '\\n'\n","            elif para.name in ['ol','ul']:\n","                for li in para.find_all('li'):\n","                    article_text += li.get_text() + '\\n'\n","\n","        return title, article_text\n","\n","    #return error for failed url\n","    except Exception as e:\n","        print(f\"Error fetching {url}: {e}\")\n","        return None, None\n","\n","def main():\n","    input_file = '/content/drive/MyDrive/Blackcoffer/Input.csv'\n","    output_dir = 'articles'\n","\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    df = pd.read_csv(input_file)\n","\n","    for index, row in df.iterrows():\n","        url_id = row['URL_ID']\n","        url = row['URL']\n","\n","        if pd.isna(url):\n","            print(f\"Skipping URL_ID {url_id} due to missing URL\")\n","            continue\n","\n","        title, article_text = fetch_article(url)\n","        if title and article_text:\n","            with open(os.path.join(output_dir, f\"{url_id}.txt\"), 'w', encoding='utf-8') as file:\n","                file.write(title + '\\n' + article_text)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxG5DaQpNWOl","executionInfo":{"status":"ok","timestamp":1717360518839,"user_tz":-330,"elapsed":134552,"user":{"displayName":"armaan sharma","userId":"08508255617465016384"}},"outputId":"ebb76a87-cb6b-402a-93ca-d1e430ef47eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error fetching https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n","Error fetching https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from textblob import TextBlob\n","\n","# Ensure nltk resources are downloaded\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","def load_stop_words(stopwords_dir):\n","    stop_words = set(stopwords.words('english'))\n","    for filename in os.listdir(stopwords_dir):\n","        if filename.endswith(\".txt\"):\n","            filepath = os.path.join(stopwords_dir, filename)\n","            # Open the file with the correct encoding\n","            with open(filepath, 'r', encoding='latin-1') as file: # Change from utf-8 to latin-1\n","                stop_words.update(file.read().splitlines())\n","    return stop_words\n","\n","stop_words = load_stop_words('/content/drive/MyDrive/Blackcoffer/StopWords')\n","\n","with open('/content/drive/MyDrive/Blackcoffer/MasterDictionary/positive-words.txt', 'r', encoding='latin-1') as f:\n","    positive_words = set(f.read().split())\n","\n","with open('/content/drive/MyDrive/Blackcoffer/MasterDictionary/negative-words.txt', 'r', encoding='latin-1') as f:\n","    negative_words = set(f.read().split())\n","\n","positive_words = positive_words - stop_words\n","negative_words = negative_words - stop_words\n","\n","def clean_tokenize_text(text):\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","    tokens = word_tokenize(text.lower())  # Tokenize and lower case\n","    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n","    return tokens\n","\n","def analyze_text(text):\n","    tokens = clean_tokenize_text(text)\n","    sentences = sent_tokenize(text)\n","\n","    positive_score = sum(1 for word in tokens if word in positive_words)\n","    negative_score = sum(1 for word in tokens if word in negative_words)\n","\n","    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n","    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n","\n","    avg_sentence_length = len(tokens) / len(sentences)\n","    complex_word_count = sum(1 for word in tokens if len(re.findall(r'[aeiouy]', word)) > 2)\n","    percentage_complex_words = (complex_word_count / len(tokens)) * 100\n","    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n","\n","    avg_words_per_sentence = len(tokens) / len(sentences)\n","    syllable_per_word = sum(len(re.findall(r'[aeiouy]', word)) for word in tokens) / len(tokens)\n","    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n","    avg_word_length = sum(len(word) for word in tokens) / len(tokens)\n","\n","    return {\n","        \"POSITIVE SCORE\": positive_score,\n","        \"NEGATIVE SCORE\": negative_score,\n","        \"POLARITY SCORE\": polarity_score,\n","        \"SUBJECTIVITY SCORE\": subjectivity_score,\n","        \"AVG SENTENCE LENGTH\": avg_sentence_length,\n","        \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n","        \"FOG INDEX\": fog_index,\n","        \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n","        \"COMPLEX WORD COUNT\": complex_word_count,\n","        \"WORD COUNT\": len(tokens),\n","        \"SYLLABLE PER WORD\": syllable_per_word,\n","        \"PERSONAL PRONOUNS\": personal_pronouns,\n","        \"AVG WORD LENGTH\": avg_word_length\n","    }\n","def main():\n","    input_file = '/content/drive/MyDrive/Blackcoffer/Input.csv'\n","    output_file = '/content/drive/MyDrive/Blackcoffer/Output Data Structure.csv'\n","    articles_dir = '/content/articles'\n","\n","    input_df = pd.read_csv(input_file)\n","\n","    output_columns = [\n","        'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n","        'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n","        'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n","        'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n","    ]\n","    results=[]\n","\n","    for index, row in input_df.iterrows():\n","        url_id = row['URL_ID']\n","        file_path = os.path.join(articles_dir, f\"{url_id}.txt\")\n","\n","        if os.path.exists(file_path):\n","            with open(file_path, 'r', encoding='latin-1') as file:\n","                text = file.read()\n","\n","            analysis_results = analyze_text(text)\n","            analysis_results['URL_ID'] = url_id\n","            analysis_results['URL'] = row['URL']\n","\n","            results.append(analysis_results)\n","\n","        else:\n","            print(f\"File {file_path} not found, skipping URL_ID {url_id}\")\n","\n","    output_df = pd.DataFrame(results, columns=output_columns)\n","    output_df.to_csv(output_file, index=False)\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"],"metadata":{"id":"o5hZaVUqceB5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717360529906,"user_tz":-330,"elapsed":1940,"user":{"displayName":"armaan sharma","userId":"08508255617465016384"}},"outputId":"ad09ec8c-8702-423f-a781-d1300c0a9e3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["File /content/articles/blackassign0036.txt not found, skipping URL_ID blackassign0036\n","File /content/articles/blackassign0049.txt not found, skipping URL_ID blackassign0049\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"poxs19ajleu_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Pdmg_cOhmfyn"},"execution_count":null,"outputs":[]}]}